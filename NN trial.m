% vector of 21 input patterns, equally spaced on the closed interval [0 1] was created with
% 'linspace' command. From the vector created, the output is generated by the following rela-
%  tionship,
% yp = sin(0:7.xp).

x=linspace(0,1,21);
y=sin(0.7*pi*x);

% a close observation of the plot can stir up the intuition that the plot has
% two patterns connected(one till 0.7 on the x-axis and another starting from 0.7 on the x-axis). A
% piecewise regression of 2 pieces appears to be an alternative.
figure
plot(x,y,':b+')
xlim()
xlabel('Input');
ylabel('Output');
title('Graph of Linear Input and Sin Output');
net = fitnet(2);
net.inputs{1}.processFcns={};
net.outputs{2}.processFcns={};
net = configure(net,x,y);
[net,tr] = train(net,x,y);
[biases_1,weights_1]=hidden_layer_weights(net);
a_temp1=weights_1(1)*x+biases_1(1);
a_temp2=weights_1(2)*x+biases_1(2);
a1=tansig(a_temp1);
a2=tansig(a_temp2);
[biases, weights] = output_layer_weights(net);
w1=det(weights(1));
w2=det(weights(2));
output=purelin(a1*w1 + a2*w2 +biases);

% As can be observed from the figure, the response has a curvature which divides into curve two
% parts. Each activation has a steep learning curve for one part of the output. When one activation
% has a steep learning, it can be observed that the other activation has low learning. This explains
% that each activation is trained to respond to one part of the curve.

figure
plot(1:21,a_temp1,'-r*',1:21,a_temp2,':g+');
xlabel('Input Patterns');
ylabel('Activation');
title('Activations')

legend('Activation 1','Activation 2');
figure
plot(output,'-bo');
xlabel('Input Patterns');
ylabel('Output');
title('Estimated Output')
